{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prime-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker.local import LocalSession\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "realistic-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oriental-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.getLevelName(\"INFO\"),\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coastal-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001\n",
      "sagemaker bucket: .\n",
      "sagemaker session region: local\n"
     ]
    }
   ],
   "source": [
    "if LOCAL:\n",
    "    session = LocalSession()\n",
    "    session.config = {\"local\": {\"local_code\": True}}\n",
    "    bucket = \".\"\n",
    "    role = \"arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001\"\n",
    "    region = \"local\"\n",
    "    train_input_path = \"file://./data/train\"\n",
    "    val_input_path = \"file://./data/val\"\n",
    "    test_input_path = \"file://./data/test\"\n",
    "\n",
    "else:\n",
    "\n",
    "    session = sagemaker.Session()\n",
    "    # sagemaker session bucket -> used for uploading data, models and logs\n",
    "    # sagemaker will automatically create this bucket if it not exists\n",
    "    bucket = \"quantsagemaker\"\n",
    "    if bucket is None and sess is not None:\n",
    "        # set to default bucket if a bucket name is not given\n",
    "        bucket = session.default_bucket()\n",
    "\n",
    "    role = sagemaker.get_execution_role()\n",
    "    session = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "    region = session.boto_region_name\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae74b6fa-e763-497a-b388-7e827118d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_path = \"s3://sagemaker-eu-central-1-611215368770/sagemaker/imdb/train.csv\"\n",
    "val_input_path = \"s3://sagemaker-eu-central-1-611215368770/sagemaker/imdb/val.csv\"\n",
    "test_input_path = \"s3://sagemaker-eu-central-1-611215368770/sagemaker/imdb/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfb00c-43f3-4810-86bc-db4f488b7baa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"epochs\": 1,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"model_name\": \"distilbert-base-uncased\",\n",
    "}\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"./code\",\n",
    "    role=role,\n",
    "    framework_version=\"1.7.1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"local\",\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "estimator.fit({'training': train_input_path, 'validating': val_input_path, 'testing': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfcf0c3-5984-415e-a642-74df58a27aa6",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8330d75-75ff-4123-9897-f82e25529bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh -s $estimator.model_data\n",
    "mkdir model\n",
    "aws s3 cp $1 model/ \n",
    "tar xvzf model/model.tar.gz --directory ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff0ad12-7477-41e9-8a99-a0a815b0d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = estimator.model_data\n",
    "print(model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d000cf-20b4-484a-b626-c1820122c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "pytorch_model = PyTorchModel(\n",
    "    model_data=\"s3://sagemaker-eu-central-1-611215368770/pytorch-training-2021-06-01-09-20-56-523/model.tar.gz\",\n",
    "    role=role,\n",
    "    framework_version=\"1.7.1\",\n",
    "    source_dir=\"code\",\n",
    "    py_version=\"py3\",\n",
    "    entry_point=\"inference.py\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e478390-2724-4257-9da6-b4b2229683c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-01 13:16:55,882 - botocore.credentials - INFO - Found credentials in environment variables.\n",
      "2021-06-01 13:18:27,863 - sagemaker - INFO - Creating model with name: pytorch-inference-2021-06-01-11-18-27-862\n"
     ]
    }
   ],
   "source": [
    "transformer = pytorch_model.transformer(instance_count=1, instance_type=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96f932a2-3ce8-4f13-a500-c3367f21036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-01 13:18:27,868 - sagemaker - INFO - Creating transform job with name: pytorch-inference-2021-06-01-11-18-27-868\n",
      "2021-06-01 13:18:27,869 - sagemaker.local.image - INFO - serving\n",
      "2021-06-01 13:18:27,870 - sagemaker.local.image - INFO - creating hosting dir in /tmp/tmp_mukpy_4\n",
      "2021-06-01 13:18:45,496 - sagemaker.local.image - WARNING - Using the short-lived AWS credentials found in session. They might expire while running.\n",
      "2021-06-01 13:18:45,552 - sagemaker.local.image - INFO - docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-myu8a:\n",
      "    command: serve\n",
      "    container_name: 1aoywkheg3-algo-1-myu8a\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-inference:1.7.1-cpu-py3\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-myu8a\n",
      "    ports:\n",
      "    - 8080:8080\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmp5wsddp7o:/opt/ml/model\n",
      "version: '2.3'\n",
      "\n",
      "2021-06-01 13:18:45,553 - sagemaker.local.image - INFO - docker command: docker-compose -f /tmp/tmp_mukpy_4/docker-compose.yaml up --build --abort-on-container-exit\n",
      "2021-06-01 13:18:45,554 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 5\n",
      "2021-06-01 13:18:45,566 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f60ad784400>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "2021-06-01 13:18:45,567 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f60ad784bb0>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "2021-06-01 13:18:45,568 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f60ad766d30>: Failed to establish a new connection: [Errno 111] Connection refused')': /ping\n",
      "2021-06-01 13:18:45,569 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "2021-06-01 13:18:50,572 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 10\n",
      "2021-06-01 13:18:50,574 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:18:50,576 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:18:50,578 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:18:50,581 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "2021-06-01 13:18:55,584 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 15\n",
      "2021-06-01 13:18:55,586 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:18:55,588 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:18:55,589 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:18:55,590 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "2021-06-01 13:19:00,596 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 20\n",
      "2021-06-01 13:19:00,597 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:00,598 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:00,599 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:00,601 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "2021-06-01 13:19:05,604 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 25\n",
      "2021-06-01 13:19:05,606 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:05,608 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:05,610 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:05,611 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "2021-06-01 13:19:10,616 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 30\n",
      "2021-06-01 13:19:10,618 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:10,618 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:10,619 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:10,620 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "Attaching to 1aoywkheg3-algo-1-myu8a\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting transformers==4.4.2\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading transformers-4.4.2-py3-none-any.whl (2.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0 MB 4.5 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting datasets[s3]==1.5.0\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading datasets-1.5.0-py3-none-any.whl (192 kB)\n",
      "\u001b[K     |████████████████████████████████| 192 kB 45.5 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting multiprocess\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading multiprocess-0.70.11.1-py36-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 28.1 MB/s ta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting importlib-metadata\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading importlib_metadata-4.4.0-py3-none-any.whl (17 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting tqdm<4.50.0,>=4.27\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 8.0 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting fsspec\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading fsspec-2021.5.0-py3-none-any.whl (111 kB)\n",
      "\u001b[K     |████████████████████████████████| 111 kB 59.5 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting pyarrow>=0.17.1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading pyarrow-4.0.1-cp36-cp36m-manylinux2014_x86_64.whl (21.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.9 MB 15.2 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.25.0)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting xxhash\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading xxhash-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 49.8 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting huggingface-hub<0.1.0\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading huggingface_hub-0.0.9-py3-none-any.whl (37 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.22.0)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.8)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (1.19.1)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting dill\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading dill-0.3.3-py2.py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 17.0 MB/s eta 0:00:01\n",
      "2021-06-01 13:19:15,624 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 35\n",
      "2021-06-01 13:19:15,626 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:15,628 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:15,629 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:15,631 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting boto3==1.16.43\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 54.0 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting botocore==1.19.43\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading botocore-1.19.43-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 26.3 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting filelock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from transformers==4.4.2->-r /opt/ml/model/code/requirements.txt (line 1)) (20.4)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting regex!=2019.12.17\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading regex-2021.4.4-cp36-cp36m-manylinux2014_x86_64.whl (722 kB)\n",
      "\u001b[K     |████████████████████████████████| 722 kB 57.2 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 29.2 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting sacremoses\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 57.9 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.3.7)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (0.10.0)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore==1.19.43->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.8.1)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.6/site-packages (from botocore==1.19.43->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (1.25.11)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.7.4.3)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.19.43->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (1.15.0)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2.8)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2020.12.5)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (3.0.4)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting s3fs\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-2021.5.0-py3-none-any.whl (24 kB)\n",
      "2021-06-01 13:19:20,636 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 40\n",
      "2021-06-01 13:19:20,638 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:20,640 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:20,642 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:20,644 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting zipp>=0.5\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading zipp-3.4.1-py3-none-any.whl (5.2 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->transformers==4.4.2->-r /opt/ml/model/code/requirements.txt (line 1)) (2.4.7)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets[s3]==1.5.0->-r /opt/ml/model/code/requirements.txt (line 2)) (2021.1)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting aiobotocore>=1.0.1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading aiobotocore-1.3.0.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 6.8 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 5.5 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.2.1.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 5.0 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.2.0.tar.gz (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.1.2-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.5 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting aiohttp>=3.3.1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading aiohttp-3.7.4.post0-cp36-cp36m-manylinux2014_x86_64.whl (1.3 MB)\n",
      "2021-06-01 13:19:25,649 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 45\n",
      "2021-06-01 13:19:25,650 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:25,651 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:25,652 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:25,654 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 15 kB/s s eta 0:00:01\n",
      "2021-06-01 13:19:30,659 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 50\n",
      "2021-06-01 13:19:30,661 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:30,664 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:30,665 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:30,667 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hCollecting aiobotocore>=1.0.1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading aiobotocore-1.1.1-py3-none-any.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 4.3 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.1.0-py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 312 kB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.0.7-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.0.6-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.0.5-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.0.4-py3-none-any.whl (41 kB)\n",
      "\u001b[K     |████████████████████████████████| 41 kB 1.2 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.0.3-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.0.2-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 10.5 MB/s eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading aiobotocore-1.0.1-py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting s3fs\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-2021.4.0-py3-none-any.whl (23 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.5.1-py3-none-any.whl (21 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.5.0-py3-none-any.whl (21 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.4.1-py3-none-any.whl (19 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.4.0-py3-none-any.whl (18 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.3.5.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 6.6 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.3.4-py3-none-any.whl (18 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.3.3.tar.gz (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.3.2.tar.gz (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.3.1.tar.gz (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 4.0 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.3.0.tar.gz (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 3.1 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.2.2.tar.gz (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 4.1 MB/s  eta 0:00:01\n",
      "2021-06-01 13:19:35,672 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 55\n",
      "2021-06-01 13:19:35,675 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:35,678 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:35,681 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:35,686 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.2.1.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 2.2 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.2.0.tar.gz (46 kB)\n",
      "\u001b[K     |████████████████████████████████| 46 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.1.6.tar.gz (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 3.2 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25h  Downloading s3fs-0.1.5.tar.gz (27 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.1.4.tar.gz (25 kB)\n",
      "2021-06-01 13:19:40,688 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 60\n",
      "2021-06-01 13:19:40,698 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:40,707 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:40,714 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:40,718 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.1.3-py2.py3-none-any.whl (17 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.1.2.tar.gz (24 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.1.1.tar.gz (24 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.1.0-py2.py3-none-any.whl (15 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.0.9-py2.py3-none-any.whl (15 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.0.8-py2.py3-none-any.whl (14 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.0.7-py2.py3-none-any.whl (14 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.0.6-py2.py3-none-any.whl (13 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.0.5-py2.py3-none-any.whl (13 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.0.4-py2.py3-none-any.whl (11 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.0.2-py2.py3-none-any.whl (10 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading s3fs-0.0.1-py2.py3-none-any.whl (10 kB)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Collecting click\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Downloading click-8.0.1-py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 7.6 MB/s  eta 0:00:01\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers==4.4.2->-r /opt/ml/model/code/requirements.txt (line 1)) (1.0.1)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Installing collected packages: zipp, tqdm, importlib-metadata, fsspec, filelock, dill, botocore, xxhash, s3fs, regex, pyarrow, multiprocess, huggingface-hub, click, tokenizers, sacremoses, datasets, boto3, transformers\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Attempting uninstall: tqdm\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m     Found existing installation: tqdm 4.59.0\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m     Uninstalling tqdm-4.59.0:\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m       Successfully uninstalled tqdm-4.59.0\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m   Attempting uninstall: botocore\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m     Found existing installation: botocore 1.20.52\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m     Uninstalling botocore-1.20.52:\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m       Successfully uninstalled botocore-1.20.52\n",
      "2021-06-01 13:19:45,724 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 65\n",
      "2021-06-01 13:19:45,726 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:45,728 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:45,730 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:45,732 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m awscli 1.19.52 requires botocore==1.20.52, but you have botocore 1.19.43 which is incompatible.\u001b[0m\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Successfully installed boto3-1.16.43 botocore-1.19.43 click-8.0.1 datasets-1.5.0 dill-0.3.3 filelock-3.0.12 fsspec-2021.5.0 huggingface-hub-0.0.9 importlib-metadata-4.4.0 multiprocess-0.70.11.1 pyarrow-4.0.1 regex-2021.4.4 s3fs-0.4.2 sacremoses-0.0.45 tokenizers-0.10.3 tqdm-4.49.0 transformers-4.4.2 xxhash-2.0.2 zipp-3.4.1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.6/site-packages/sagemaker_pytorch_serving_container/etc/log4j.properties', '--models', 'model.mar']\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:50,635 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Torchserve version: 0.3.1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m TS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Current directory: /\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Max heap size: 8002 M\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Python executable: /opt/conda/bin/python3.6\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Config file: /etc/sagemaker-ts.properties\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Metrics address: http://127.0.0.1:8082\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Model Store: /.sagemaker/ts/models\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Initial Models: model.mar\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Log dir: /logs\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Netty threads: 0\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Netty client threads: 0\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Default workers per model: 8\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Allowed Urls: [file://.*|http(s)?://.*]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Custom python dependency for model allowed: false\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Metrics report format: prometheus\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Enable metrics API: true\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:50,673 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: model.mar\n",
      "2021-06-01 13:19:50,738 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 70\n",
      "2021-06-01 13:19:50,741 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:50,743 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:50,746 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:50,748 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "2021-06-01 13:19:55,752 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 75\n",
      "2021-06-01 13:19:55,754 - urllib3.connectionpool - WARNING - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:55,755 - urllib3.connectionpool - WARNING - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /ping\n",
      "2021-06-01 13:19:55,758 - urllib3.connectionpool - WARNING - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "2021-06-01 13:19:55,760 - sagemaker.local.entities - INFO - Container still not up, got: -1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:58,948 [INFO ] main org.pytorch.serve.archive.ModelArchive - eTag 366683cb3a354ea8836cf5b510facd7b\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:58,959 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:58,974 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,321 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9004\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,343 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,338 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,322 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9006\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,346 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]118\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,346 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]114\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,346 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,347 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,347 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,347 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,348 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]116\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,348 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]121\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,348 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,354 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,354 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9005\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,355 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,355 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,355 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9007\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,358 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,359 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]115\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,359 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]120\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,359 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,359 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,360 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,360 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,360 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]117\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,360 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,360 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,361 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,361 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,362 [INFO ] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,363 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,363 [INFO ] W-9006-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,363 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,363 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,363 [INFO ] W-9007-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,363 [INFO ] W-9005-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,363 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,367 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,369 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - [PID]119\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,370 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Torch worker started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,370 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,370 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,387 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,387 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9004.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,388 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9006.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,388 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,388 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,387 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,388 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9005.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:19:59,388 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.ts.sock.9007.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m Model server started.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:00,180 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:01435cc45745,timestamp:1622546400\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:00,181 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:74.6563491821289|#Level:Host|#hostname:01435cc45745,timestamp:1622546400\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:00,182 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:145.05967712402344|#Level:Host|#hostname:01435cc45745,timestamp:1622546400\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:00,182 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:66.0|#Level:Host|#hostname:01435cc45745,timestamp:1622546400\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:00,183 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:18915.52734375|#Level:Host|#hostname:01435cc45745,timestamp:1622546400\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:00,184 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:11201.109375|#Level:Host|#hostname:01435cc45745,timestamp:1622546400\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:00,186 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:40.9|#Level:Host|#hostname:01435cc45745,timestamp:1622546400\n",
      "2021-06-01 13:20:00,764 - sagemaker.local.entities - INFO - Checking if serving container is up, attempt: 80\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,225 [INFO ] pool-1-thread-9 ACCESS_LOG - /172.18.0.1:60148 \"GET /ping HTTP/1.1\" 200 118\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,226 [INFO ] pool-1-thread-9 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,272 [INFO ] epollEventLoopGroup-3-2 ACCESS_LOG - /172.18.0.1:60152 \"GET /execution-parameters HTTP/1.1\" 404 3\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,277 [INFO ] epollEventLoopGroup-3-2 TS_METRICS - Requests4XX.Count:1|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,493 [INFO ] W-9002-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,534 [INFO ] W-9006-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,589 [INFO ] W-9007-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,810 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:01,919 [INFO ] W-9003-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:02,106 [INFO ] W-9000-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:02,192 [INFO ] W-9005-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:02,281 [INFO ] W-9001-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Generating new fontManager, this may take some time...\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:11,555 [INFO ] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 12086\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:11,556 [INFO ] W-9004-model_1 TS_METRICS - W-9004-model_1.ms:12587|#Level:Host|#hostname:01435cc45745,timestamp:1622546411\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:11,556 [INFO ] W-9004-model_1 TS_METRICS - WorkerThreadTime.ms:75|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,055 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Lock 140136533493464 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,525 [INFO ] W-9005-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13056\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,525 [INFO ] W-9005-model_1 TS_METRICS - W-9005-model_1.ms:13556|#Level:Host|#hostname:01435cc45745,timestamp:1622546412\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,525 [INFO ] W-9005-model_1 TS_METRICS - WorkerThreadTime.ms:74|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,534 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - /opt/ml/model/code/inference.py:61: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,535 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle -   df.column = ['text', 'label']\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,535 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,538 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Lock 140136533493464 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,538 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,837 [INFO ] W-9007-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13376\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,838 [INFO ] W-9007-model_1 TS_METRICS - W-9007-model_1.ms:13869|#Level:Host|#hostname:01435cc45745,timestamp:1622546412\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,838 [INFO ] W-9007-model_1 TS_METRICS - WorkerThreadTime.ms:67|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:12,995 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Lock 140135518780776 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,018 [INFO ] W-9000-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13548\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,019 [INFO ] W-9000-model_1 TS_METRICS - W-9000-model_1.ms:14053|#Level:Host|#hostname:01435cc45745,timestamp:1622546413\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,020 [INFO ] W-9000-model_1 TS_METRICS - WorkerThreadTime.ms:77|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,059 [INFO ] W-9006-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13590\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,059 [INFO ] W-9006-model_1 TS_METRICS - W-9006-model_1.ms:14090|#Level:Host|#hostname:01435cc45745,timestamp:1622546413\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,060 [INFO ] W-9006-model_1 TS_METRICS - WorkerThreadTime.ms:75|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,107 [INFO ] W-9002-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13638\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,107 [INFO ] W-9002-model_1 TS_METRICS - W-9002-model_1.ms:14139|#Level:Host|#hostname:01435cc45745,timestamp:1622546413\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,108 [INFO ] W-9002-model_1 TS_METRICS - WorkerThreadTime.ms:75|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,175 [INFO ] W-9001-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13706\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,175 [INFO ] W-9001-model_1 TS_METRICS - W-9001-model_1.ms:14207|#Level:Host|#hostname:01435cc45745,timestamp:1622546413\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,175 [INFO ] W-9001-model_1 TS_METRICS - WorkerThreadTime.ms:74|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,214 [INFO ] W-9003-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 13757\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,215 [INFO ] W-9003-model_1 TS_METRICS - W-9003-model_1.ms:14246|#Level:Host|#hostname:01435cc45745,timestamp:1622546413\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,216 [INFO ] W-9003-model_1 TS_METRICS - WorkerThreadTime.ms:64|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,459 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 442/442 [00:00<00:00, 136kB/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,459 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,564 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,652 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  50%|████▉     | 115k/232k [00:00<00:00, 1.10MB/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:13,652 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Lock 140135518780776 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:14,152 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Lock 140135779860832 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:14,607 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 232k/232k [00:00<00:00, 1.20MB/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:14,607 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:14,711 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:14,811 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  22%|██▏       | 102k/466k [00:00<00:00, 989kB/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:14,893 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:  74%|███████▍  | 345k/466k [00:00<00:00, 1.20MB/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:14,894 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Lock 140135779860832 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:16,345 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Lock 140135779860832 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:16,841 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 466k/466k [00:00<00:00, 1.63MB/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:16,842 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - \n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:16,845 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:20:16,847 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Lock 140135779860832 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:00,201 [INFO ] pool-2-thread-1 TS_METRICS - CPUUtilization.Percent:0.0|#Level:Host|#hostname:01435cc45745,timestamp:1622546460\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:00,202 [INFO ] pool-2-thread-1 TS_METRICS - DiskAvailable.Gigabytes:74.65565490722656|#Level:Host|#hostname:01435cc45745,timestamp:1622546460\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:00,203 [INFO ] pool-2-thread-1 TS_METRICS - DiskUsage.Gigabytes:145.06037139892578|#Level:Host|#hostname:01435cc45745,timestamp:1622546460\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:00,206 [INFO ] pool-2-thread-1 TS_METRICS - DiskUtilization.Percent:66.0|#Level:Host|#hostname:01435cc45745,timestamp:1622546460\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:00,208 [INFO ] pool-2-thread-1 TS_METRICS - MemoryAvailable.Megabytes:12930.9609375|#Level:Host|#hostname:01435cc45745,timestamp:1622546460\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:00,210 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUsed.Megabytes:17181.55859375|#Level:Host|#hostname:01435cc45745,timestamp:1622546460\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:00,211 [INFO ] pool-2-thread-1 TS_METRICS - MemoryUtilization.Percent:59.6|#Level:Host|#hostname:01435cc45745,timestamp:1622546460\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,561 [INFO ] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Backend response time: 60000\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,561 [ERROR] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Number or consecutive unsuccessful inference 1\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,563 [ERROR] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Backend worker error\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m org.pytorch.serve.wlm.WorkerInitializationException: Backend worker did not respond in given time\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:198)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m \tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,575 [INFO ] epollEventLoopGroup-5-5 org.pytorch.serve.wlm.WorkerThread - 9004 Worker disconnected. WORKER_MODEL_LOADED\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,576 [INFO ] W-9004-model_1 ACCESS_LOG - /172.18.0.1:60158 \"POST /invocations HTTP/1.1\" 500 69754\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,581 [INFO ] W-9004-model_1 TS_METRICS - Requests5XX.Count:1|#Level:Host|#hostname:01435cc45745,timestamp:null\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,585 [WARN ] W-9004-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1-stderr\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,585 [WARN ] W-9004-model_1 org.pytorch.serve.wlm.WorkerLifeCycle - terminateIOStreams() threadName=W-9004-model_1-stdout\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,588 [INFO ] W-9004-model_1 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9004 in 1 seconds.\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,732 [WARN ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 8.86kB/s]\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,732 [INFO ] W-9004-model_1-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1-stdout\n",
      "\u001b[36m1aoywkheg3-algo-1-myu8a |\u001b[0m 2021-06-01 11:21:11,733 [INFO ] W-9004-model_1-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9004-model_1-stderr\n",
      "Gracefully stopping... (press Ctrl+C again to force)\n",
      "."
     ]
    }
   ],
   "source": [
    "transformer.transform(test_input_path, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "354dfaed-84ce-4bf5-a1fa-a6416a99fe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-eu-central-1-611215368770/pytorch-inference-2021-06-01-11-18-27-868/pytorch-inference-2021-06-01-11-18-27-868/test.csv.out to pytorch-inference-2021-06-01-11-18-27-868/test.csv.out\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $transformer.output_path ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b507f9b-3c88-4a3d-bfdd-ecbfec26cf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-01 13:25:46,919 - botocore.credentials - INFO - Found credentials in environment variables.\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ExpiredToken) when calling the GetCallerIdentity operation: The security token included in the request is expired",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6f2d330dfbc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpytorch_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeserializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringDeserializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'this is a very good movie'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/email-classifier-service_2/.venv/lib/python3.8/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"-\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_model_suffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_sagemaker_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         production_variant = sagemaker.production_variant(\n\u001b[1;32m    765\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/email-classifier-service_2/.venv/lib/python3.8/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36m_create_sagemaker_model\u001b[0;34m(self, instance_type, accelerator_type, tags)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlatest\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mservices\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;31m#SageMaker.Client.add_tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0mcontainer_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_container_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_base_name_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontainer_def\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Image\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/email-classifier-service_2/.venv/lib/python3.8/site-packages/sagemaker/pytorch/model.py\u001b[0m in \u001b[0;36mprepare_container_def\u001b[0;34m(self, instance_type, accelerator_type)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mdeploy_key_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_code_key_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeploy_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upload_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeploy_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_mms_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mdeploy_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mdeploy_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_framework_env_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/email-classifier-service_2/.venv/lib/python3.8/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36m_upload_code\u001b[0;34m(self, key_prefix, repack)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m             \u001b[0mbucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_bucket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m             \u001b[0mrepacked_model_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"s3://\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.tar.gz\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/email-classifier-service_2/.venv/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mdefault_bucket\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mdefault_bucket\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_default_bucket_name_override\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefault_bucket\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             account = self.boto_session.client(\n\u001b[0m\u001b[1;32m    372\u001b[0m                 \u001b[0;34m\"sts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregion_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msts_regional_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m             ).get_caller_identity()[\"Account\"]\n",
      "\u001b[0;32m~/PycharmProjects/email-classifier-service_2/.venv/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/email-classifier-service_2/.venv/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ExpiredToken) when calling the GetCallerIdentity operation: The security token included in the request is expired"
     ]
    }
   ],
   "source": [
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type=\"local\")\n",
    "predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "predictor.deserializer = sagemaker.deserializers.StringDeserializer()\n",
    "predictor.predict('this is a very good movie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe580d5-a87b-447d-82b0-c7fae71e3bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(['this movie sucks', 'this movie is ok'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dab355-0034-48f1-8dac-54c273d1ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.predict(['such a terrible movie', 'what a great movie', 'omg best movie ever'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4813e3f-5aac-45d5-bb5b-4b3017decca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99a271c-d422-472c-b8d3-3394aad9326b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, AutoConfig\n",
    "config = AutoConfig.from_pretrained(os.path.join('model', 'config.json'))\n",
    "model = AutoModelForSequenceClassification.from_pretrained(os.path.join('model', 'pytorch_model.bin'),\n",
    "                                                               config=config)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151b8c9e-3445-43fc-8b29-6a7e75ef5bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['this is a terrific movie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b610218-c382-4c6f-8cfe-44b4ccea08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenizer(inputs, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36600c8b-7013-4625-91ca-5b3449bee7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.Tensor(tokenized_input['input_ids']).long()\n",
    "attention_mask = torch.Tensor(tokenized_input['attention_mask']).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34cd9c8-030a-4f71-ac26-2f4eb1a5e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    res = model(input_ids, attention_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
