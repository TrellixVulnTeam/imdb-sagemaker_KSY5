{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prime-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39b467-40e5-4674-adbf-be881ab3b00c",
   "metadata": {},
   "source": [
    "# A SageMaker Workflow\n",
    "\n",
    "The pipeline that we create follows a typical Machine Learning Application pattern of pre-processing, training, evaluation, and model registration:\n",
    "\n",
    "![A typical ML Application pipeline](img/pipeline-full.png)\n",
    "\n",
    "### Create SageMaker Clients and Session\n",
    "\n",
    "First, we create a new SageMaker Session in the current AWS region. We also acquire the role arn for the session.\n",
    "\n",
    "This role arn should be the execution role arn that you set up in the Prerequisites section of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coastal-hudson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::611215368770:role/ScalableCapitalMothership\n",
      "sagemaker bucket: sagemaker-eu-central-1-611215368770\n",
      "sagemaker session region: eu-central-1\n"
     ]
    }
   ],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = \"eu-central-1\"\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce638290-c112-40b9-87a0-3468546274af",
   "metadata": {},
   "source": [
    "## BYO Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92892285-b6b1-4f5a-9dc6-2ad0598220b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "ecr_repository = \"sagemaker-pytorch-processing-container\"\n",
    "tag = \":latest\"\n",
    "\n",
    "uri_suffix = \"amazonaws.com\"\n",
    "processing_repository_uri = f\"{account_id}.dkr.ecr.{region}.{uri_suffix}/{ecr_repository + tag}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4ca3c-6203-44da-85d7-5033d758093d",
   "metadata": {},
   "source": [
    "# Track the Pipeline as an `Experiment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b264e0f-d179-47e3-a05b-93d189c2415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "timestamp = int(time.time())\n",
    "\n",
    "pipeline_name = f\"imdb-pipeline-{timestamp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f0bead-e593-47ef-ac3b-f388c5a6417c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d96314-2ba5-4984-9409-32eed1424993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline experiment name: imdb-pipeline-1623848188\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "pipeline_experiment = Experiment.create(\n",
    "    experiment_name=pipeline_name,\n",
    "    description=\"Imdb Reviews DistillBert Pipeline Experiment\",\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "pipeline_experiment_name = pipeline_experiment.experiment_name\n",
    "print(f\"Pipeline experiment name: {pipeline_experiment_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c37097-af72-43c9-afab-d6369a47d2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_experiment_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_experiment_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb947e4-e5ec-405b-8402-0ecc10f4a34e",
   "metadata": {},
   "source": [
    "# Create the `Trial`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c65fdf9d-eeb3-4800-a915-4dfa89a60153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial name: trial-1623848188\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.trial import Trial\n",
    "\n",
    "pipeline_trial = Trial.create(\n",
    "    trial_name=f\"trial-{timestamp}\",\n",
    "    experiment_name=pipeline_experiment_name,\n",
    "    sagemaker_boto_client=sm,\n",
    ")\n",
    "\n",
    "pipeline_trial_name = pipeline_trial.trial_name\n",
    "print(f\"Trial name: {pipeline_trial_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e76f73b-c508-4a82-b114-7833780a82fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_trial_name' (str)\n"
     ]
    }
   ],
   "source": [
    "%store pipeline_trial_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3162d7a6-e5c5-4863-b82e-172b518f081c",
   "metadata": {},
   "source": [
    "### Define Parameters to Parametrize Pipeline Execution\n",
    "\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* `ParameterString` - represents a `str` Python type\n",
    "* `ParameterInteger` - represents an `int` Python type\n",
    "* `ParameterFloat` - represents a `float` Python type\n",
    "\n",
    "These parameters support providing a default value, which can be overridden on pipeline execution. The default value specified should be an instance of the type of the parameter.\n",
    "\n",
    "The parameters defined in this workflow include:\n",
    "\n",
    "* `processing_instance_type` - The `ml.*` instance type of the processing job.\n",
    "* `processing_instance_count` - The instance count of the processing job.\n",
    "* `training_instance_type` - The `ml.*` instance type of the training job.\n",
    "* `model_approval_status` - What approval status to register the trained model with for CI/CD purposes ( \"PendingManualApproval\" is the default).\n",
    "* `input_data` - The S3 bucket URI location of the input data\n",
    "* `batch_data` - The S3 bucket URI location of the batch data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0db065-858a-4146-8fb9-0f740770a7de",
   "metadata": {},
   "source": [
    "# Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29901202-a3ce-4e64-bd32-6cc5ce4e1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d82cbc-586b-4ca6-bfb3-b035e7d0e78e",
   "metadata": {},
   "source": [
    "# Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "456b78d0-e545-4093-80a9-511212c11bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_experiment_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da16c21-d250-4220-ac59-3e6f5da0f366",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = ParameterString(\n",
    "    name=\"ExperimentName\",\n",
    "    default_value=pipeline_experiment_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4be7e-f54a-4013-84cd-d5ccc28a1333",
   "metadata": {},
   "source": [
    "# Processing Step Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e09b6-cb47-4a38-a8aa-b7d0a3be71db",
   "metadata": {},
   "source": [
    "![Define a Processing Step for Feature Engineering](img/pipeline-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01bb8279-c7c0-4b0b-ab92-f287f660c398",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input_uri = f\"s3://{bucket}/imdb/processing/raw/small/raw.csv\"\n",
    "batch_input_uri = f\"s3://{bucket}/imdb/batch/input/small/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86607da3-e981-4a6c-aae6-63649dc68379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-03 17:58:38     272610 raw.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $raw_input_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d72fcb-e051-4ff9-af79-98f9125e3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "\n",
    "processing_instance_count = ParameterInteger(\n",
    "    name=\"ProcessingInstanceCount\", default_value=1\n",
    ")\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.t3.medium\"\n",
    ")\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\", default_value=\"ml.g4dn.xlarge\"\n",
    ")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=raw_input_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4b7cd-53b0-4325-a0fc-9f7d8b18dc9d",
   "metadata": {},
   "source": [
    "### Define a Processing Step for Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5963f8e-a47d-4a8b-8686-dcb88bc84603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=processing_repository_uri,\n",
    "    role=role,\n",
    "    instance_count=processing_instance_count,\n",
    "    instance_type=processing_instance_type,\n",
    "    base_job_name=\"imdb-preprocess\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a7724db-3e1d-42ff-98e0-adea7a0ced48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingStep(name='ImdbProcess', step_type=<StepTypeEnum.PROCESSING: 'Processing'>, depends_on=None)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name=\"ImdbProcess\",\n",
    "    processor=script_processor,\n",
    "    inputs=[ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"source/preprocessing.py\",\n",
    "    job_arguments=[\"--train-test-split-ratio\", \"0.2\", \"--model_name\", \"distilbert-base-uncased\"],\n",
    ")\n",
    "\n",
    "print(step_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1aca58-0ae4-4481-b7a5-dede37916274",
   "metadata": {},
   "source": [
    "# Setup Training Hyper-Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c9f346-32ac-4e20-9c24-2814f01b0365",
   "metadata": {},
   "source": [
    "![Define a Training Step to Train a Model](img/pipeline-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7b83aa3-a5cf-4221-a2b6-6105fa3c8f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = ParameterInteger(name=\"Epochs\", default_value=1)\n",
    "\n",
    "train_batch_size = ParameterInteger(name=\"TrainBatchSize\", default_value=32)\n",
    "\n",
    "test_batch_size = ParameterInteger(name=\"TestBatchSize\", default_value=32)\n",
    "\n",
    "transformer_model_name = ParameterString(name='TransformerModelName', default_value=\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd853ba-2ef8-4051-9dae-15a48e54a270",
   "metadata": {},
   "source": [
    "# Setup Metrics To Track Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ef230e7-ca3d-47f6-9d61-b4e15a960fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mIMDbDataset\u001b[39;49;00m(torch.utils.data.Dataset):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, encodings, labels):\n",
      "        \u001b[36mself\u001b[39;49;00m.encodings = encodings\n",
      "        \u001b[36mself\u001b[39;49;00m.labels = labels\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__getitem__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, idx):\n",
      "        item = {key: torch.tensor(val[idx]) \u001b[34mfor\u001b[39;49;00m key, val \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.encodings.items()}\n",
      "        item[\u001b[33m'\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] = torch.tensor(\u001b[36mself\u001b[39;49;00m.labels[idx])\n",
      "        \u001b[34mreturn\u001b[39;49;00m item\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__len__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.labels)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\n",
      "    labels = pred.label_ids\n",
      "    preds = pred.predictions.argmax(-\u001b[34m1\u001b[39;49;00m)\n",
      "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u001b[33m\"\u001b[39;49;00m\u001b[33mbinary\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    acc = accuracy_score(labels, preds)\n",
      "    \u001b[34mreturn\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: acc, \u001b[33m\"\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: f1, \u001b[33m\"\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: precision, \u001b[33m\"\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: recall}\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval-batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# Set up logging\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "    logging.basicConfig(\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# Load model and tokenizer\u001b[39;49;00m\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)\n",
      "\n",
      "    \u001b[37m# Load dataset\u001b[39;49;00m\n",
      "    ii_train = torch.load(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mii_train.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    am_train = torch.load(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33mam_train.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    train_encodings = {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: ii_train, \u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: am_train}\n",
      "    train_labels = torch.load(os.path.join(args.train, \u001b[33m'\u001b[39;49;00m\u001b[33my_train.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\n",
      "    ii_test = torch.load(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33mii_test.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    am_test = torch.load(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33mam_test.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    test_encodings = {\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: ii_test, \u001b[33m'\u001b[39;49;00m\u001b[33mattention_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: am_test}\n",
      "    test_labels = torch.load(os.path.join(args.test, \u001b[33m'\u001b[39;49;00m\u001b[33my_test.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\n",
      "    \u001b[37m# Preprocess train dataset\u001b[39;49;00m\n",
      "    train_dataset = IMDbDataset(train_encodings, train_labels)\n",
      "    test_dataset = IMDbDataset(test_encodings, test_labels)\n",
      "\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.model_dir,\n",
      "        num_train_epochs=args.epochs,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saves the model to s3\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize source/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a301e9-c6b9-4a00-b631-3b90e67f847d",
   "metadata": {},
   "source": [
    "# Define a Training Step to Train a Model\n",
    "\n",
    "We configure an Estimator and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later.\n",
    "\n",
    "We also specify the model path where the models from training will be saved.\n",
    "\n",
    "Note the `train_instance_type` parameter passed may be also used and passed into other places in the pipeline. In this case, the `train_instance_type` is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "addd0a59-1779-46dc-852e-1bb56590724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "model_path = f\"s3://{bucket}/imdb/model_train\"\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": epochs,\n",
    "    \"train_batch_size\": train_batch_size,\n",
    "    \"model_name\": transformer_model_name,\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"source\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    framework_version=\"1.6\",\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c36fe12-d41d-43cc-b2d2-a2866267ea76",
   "metadata": {},
   "source": [
    "### Setup Pipeline Step Caching\n",
    "Cache pipeline steps for a duration of time using [ISO 8601](https://en.wikipedia.org/wiki/ISO_8601#Durations) format.  \n",
    "\n",
    "More details on SageMaker Pipeline step caching here:  https://docs.aws.amazon.com/sagemaker/latest/dg/pipelines-caching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d1004a3-93c3-4ca7-9865-44707ca51679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"PT1H\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90f9d393-8aa5-414d-9c52-d4f36712f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingStep(name='ImdbTrain', step_type=<StepTypeEnum.TRAINING: 'Training'>, depends_on=None)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"ImdbTrain\",\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    },\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "print(step_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f31684-e8aa-4de9-ab2d-b635760fd4d8",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d20e1-6195-4211-8943-40a66e8e4c98",
   "metadata": {},
   "source": [
    "![Define a Model Evaluation Step to Evaluate the Trained Model](img/pipeline-4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9edf1228-4819-47b7-af03-a20f23a29038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProcessingStep(name='ImdbEval', step_type=<StepTypeEnum.PROCESSING: 'Processing'>, depends_on=None)\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"ImdbEval\",\n",
    "    processor=script_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"source/evaluation.py\",\n",
    "    property_files=[evaluation_report],\n",
    ")\n",
    "\n",
    "print(step_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1879a25-6ba2-4096-89c0-a85da2c28374",
   "metadata": {},
   "source": [
    "# Register Model Step\n",
    "\n",
    "![](img/pipeline-5.png)\n",
    "\n",
    "We use the estimator instance that was used for the training step to construct an instance of `RegisterModel`. The result of executing `RegisterModel` in a pipeline is a Model Package. A Model Package is a reusable model artifacts abstraction that packages all ingredients necessary for inference. Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A Model Package Group is a collection of Model Packages. You can create a Model Package Group for a specific ML business problem, and you can keep adding versions/model packages into it. Typically, we expect customers to create a ModelPackageGroup for a SageMaker Workflow Pipeline so that they can keep adding versions/model packages to the group for every Workflow Pipeline run.\n",
    "\n",
    "The construction of `RegisterModel` is very similar to an estimator instance's `register` method, for those familiar with the existing Python SDK.\n",
    "\n",
    "In particular, we pass in the `S3ModelArtifacts` from the `TrainingStep`, `step_train` properties. The `TrainingStep` `properties` attribute matches the object model of the [DescribeTrainingJob](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeTrainingJob.html) response object.\n",
    "\n",
    "Of note, we provided a specific model package group name which we will use in the Model Registry and CI/CD work later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "879f84bf-4984-4d80-be04-5e608038d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sagemaker.model_metrics.ModelMetrics object at 0x7f6faa0da3d0>\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=f\"\"\"{\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        }/evaluation.json\"\"\",\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "print(model_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92d40abd-827a-4023-9c7d-937e5a6794e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\")\n",
    "\n",
    "deploy_instance_type = ParameterString(name=\"DeployInstanceType\", default_value=\"ml.t2.medium\")\n",
    "\n",
    "deploy_instance_count = ParameterInteger(name=\"DeployInstanceCount\", default_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5234fcba-84eb-4058-8af0-b8ae1fc89ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imdb-Reviews-1623848188\n"
     ]
    }
   ],
   "source": [
    "model_package_group_name = f\"Imdb-Reviews-{timestamp}\"\n",
    "\n",
    "print(model_package_group_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c579c0b-178e-44ca-9837-e855858d7e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-inference:1.6.0-cpu-py36\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"1.6.0\",\n",
    "    py_version=\"py36\",\n",
    "    instance_type=deploy_instance_type,\n",
    "    image_scope=\"inference\",\n",
    ")\n",
    "print(inference_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dfa77e63-6e55-4eff-8d9e-44351e111dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=f\"\"\"{step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]}/evaluation.json\"\"\",\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "step_register = RegisterModel(\n",
    "    name=\"ImdbRegisterModel\",\n",
    "    estimator=estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    image_uri=inference_image_uri,  # we have to specify, by default it's using training image\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[deploy_instance_type],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca42113f-0f1d-48c9-910e-34269cc2b6c1",
   "metadata": {},
   "source": [
    "# Create Model for Deployment Step\n",
    "\n",
    "![](img/pipeline-5.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7784d2a3-2a03-4693-91e3-d71d23b3d6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model_name = f\"imdb-model-{timestamp}\"\n",
    "\n",
    "model = Model(\n",
    "    name=model_name,\n",
    "    image_uri=inference_image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=session,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7af66-54bb-4c55-8e8e-fbe02c88e59b",
   "metadata": {},
   "source": [
    "Supply the model input -- `instance_type` and `accelerator_type` for creating the SageMaker Model and then define the `CreateModelStep` passing in the inputs and the model instance defined before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9921f869-f89d-43a8-9cac-7cbcd14c711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "\n",
    "\n",
    "inputs = CreateModelInput(\n",
    "    instance_type=deploy_instance_type,\n",
    ")\n",
    "step_create_model = CreateModelStep(\n",
    "    name=\"ImdbModel\",\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c4a8ca-c538-4dab-8356-c9a2ea02d7a1",
   "metadata": {},
   "source": [
    "### Define a Pipeline of Parameters, Steps, and Conditions\n",
    "\n",
    "In this section, combine the steps into a Pipeline so it can be executed.\n",
    "\n",
    "A pipeline requires a `name`, `parameters`, and `steps`. Names must be unique within an `(account, region)` pair.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All of the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline do not have to be listed in the order of execution. The SageMaker Pipeline service resolves the _data dependency_ DAG as steps for the execution to complete.\n",
    "* Steps must be unique to across the pipeline step list and all condition step if/else lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "488099aa-b8d5-4594-bdc0-a79b87a0751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "55c4f93b-12f0-41a8-a627-8a1b1a21bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pipeline_name = f\"ImdbPipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_register, step_create_model],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857902d9-2286-48f1-acd3-4109991a6fb7",
   "metadata": {},
   "source": [
    "#### (Optional) Examining the pipeline definition\n",
    "\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and the parameters and step properties resolve correctly.\n",
    "\n",
    "Note: Doesn't seem to work with FrameworkModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d346ab72-975b-49c3-8db1-6279f720d4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [{'Name': 'ProcessingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.t3.medium'},\n",
       "  {'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1},\n",
       "  {'Name': 'TrainingInstanceType',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'ml.g4dn.xlarge'},\n",
       "  {'Name': 'ModelApprovalStatus',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 'PendingManualApproval'},\n",
       "  {'Name': 'InputData',\n",
       "   'Type': 'String',\n",
       "   'DefaultValue': 's3://sagemaker-eu-central-1-611215368770/imdb/processing/raw/small/raw.csv'}],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'ImdbProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '611215368770.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-pytorch-processing-container:latest',\n",
       "     'ContainerArguments': ['--train-test-split-ratio',\n",
       "      '0.2',\n",
       "      '--model_name',\n",
       "      'distilbert-base-uncased'],\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/preprocessing.py']},\n",
       "    'RoleArn': 'arn:aws:iam::611215368770:role/ScalableCapitalMothership',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'},\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-611215368770/imdb-preprocess-2021-06-16-13-29-02-884/input/code/preprocessing.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-central-1-611215368770/imdb-preprocess-2021-06-16-13-01-52-590/output/train',\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-central-1-611215368770/imdb-preprocess-2021-06-16-13-01-52-590/output/test',\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'ImdbTrain',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-training:1.6-gpu-py36',\n",
       "     'EnableSageMakerMetricsTimeSeries': True},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-eu-central-1-611215368770/imdb/model_train'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'InstanceCount': 1,\n",
       "     'InstanceType': {'Get': 'Parameters.TrainingInstanceType'},\n",
       "     'VolumeSizeInGB': 30},\n",
       "    'RoleArn': 'arn:aws:iam::611215368770:role/ScalableCapitalMothership',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.ImdbProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.ImdbProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'epochs': {'Get': 'Parameters.Epochs'},\n",
       "     'train_batch_size': {'Get': 'Parameters.TrainBatchSize'},\n",
       "     'model_name': {'Get': 'Parameters.TransformerModelName'},\n",
       "     'sagemaker_submit_directory': '\"s3://sagemaker-eu-central-1-611215368770/pytorch-training-2021-06-16-13-29-03-151/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"train.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_job_name': '\"pytorch-training-2021-06-16-13-29-03-151\"',\n",
       "     'sagemaker_region': '\"eu-central-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-eu-central-1-611215368770/imdb/model_train',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerRuleConfigurations': [{'RuleConfigurationName': 'ProfilerReport-1623850143',\n",
       "      'RuleEvaluatorImage': '482524230118.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-debugger-rules:latest',\n",
       "      'RuleParameters': {'rule_to_invoke': 'ProfilerReport'}}],\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-eu-central-1-611215368770/imdb/model_train'}},\n",
       "   'CacheConfig': {'Enabled': True, 'ExpireAfter': 'PT1H'}},\n",
       "  {'Name': 'ImdbEval',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': {'Get': 'Parameters.ProcessingInstanceType'},\n",
       "      'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'},\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '611215368770.dkr.ecr.eu-central-1.amazonaws.com/sagemaker-pytorch-processing-container:latest',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/evaluation.py']},\n",
       "    'RoleArn': 'arn:aws:iam::611215368770:role/ScalableCapitalMothership',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.ImdbTrain.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.ImdbProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-611215368770/imdb-preprocess-2021-06-16-13-29-03-479/input/code/evaluation.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': 's3://sagemaker-eu-central-1-611215368770/imdb-preprocess-2021-06-16-13-07-27-077/output/evaluation',\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'EvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]},\n",
       "  {'Name': 'ImdbRegisterModel',\n",
       "   'Type': 'RegisterModel',\n",
       "   'Arguments': {'ModelPackageGroupName': 'Imdb-Reviews-1623848188',\n",
       "    'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json',\n",
       "       'S3Uri': 's3://sagemaker-eu-central-1-611215368770/imdb-preprocess-2021-06-16-13-07-27-077/output/evaluation/evaluation.json'}}},\n",
       "    'InferenceSpecification': {'Containers': [{'Image': '763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-inference:1.6.0-cpu-py36',\n",
       "       'ModelDataUrl': {'Get': 'Steps.ImdbTrain.ModelArtifacts.S3ModelArtifacts'}}],\n",
       "     'SupportedContentTypes': ['text/csv'],\n",
       "     'SupportedResponseMIMETypes': ['text/csv'],\n",
       "     'SupportedRealtimeInferenceInstanceTypes': [{'Get': 'Parameters.DeployInstanceType'}],\n",
       "     'SupportedTransformInstanceTypes': ['ml.m5.large']},\n",
       "    'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'}}},\n",
       "  {'Name': 'ImdbModel',\n",
       "   'Type': 'Model',\n",
       "   'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::611215368770:role/ScalableCapitalMothership',\n",
       "    'PrimaryContainer': {'Image': '763104351884.dkr.ecr.eu-central-1.amazonaws.com/pytorch-inference:1.6.0-cpu-py36',\n",
       "     'Environment': {},\n",
       "     'ModelDataUrl': {'Get': 'Steps.ImdbTrain.ModelArtifacts.S3ModelArtifacts'}}}}]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f82d6f-9093-47a9-afdc-81ea1ac5ae1a",
   "metadata": {},
   "source": [
    "### Submit the pipeline to SageMaker and start execution\n",
    "\n",
    "Submit the pipeline definition to the Pipeline service. The role passed in will be used by the Pipeline service to create all the jobs defined in the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc8879e-e267-4109-a3eb-8fd6de1acc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c846b7-f23a-4773-b3cd-a183206b0b13",
   "metadata": {},
   "source": [
    "## Ignore the `WARNING` below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0009387-a9c7-46c9-bf6e-5a08dd4e3fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = pipeline.create(role_arn=role)\n",
    "\n",
    "pipeline_arn = response[\"PipelineArn\"]\n",
    "print(pipeline_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c33a35-93d9-4fa3-8377-4085c2783d3a",
   "metadata": {},
   "source": [
    "Start the pipeline and accept all of the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba482198-012f-4c33-a6c7-80b043ba6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a9c7b-fff5-4268-ba99-2ca302d3b0e8",
   "metadata": {},
   "source": [
    "### Workflow Operations: examining and waiting for pipeline execution\n",
    "\n",
    "Now we describe execution instance and list the steps in the execution to find out more about the execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9f22c2-2f58-4cc0-81d7-37c1519bc754",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "execution_run = execution.describe()\n",
    "pprint(execution_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793de044-554c-479f-a850-ab18ed5a498f",
   "metadata": {},
   "source": [
    "# Add Execution Run as Trial to Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e5d1f-753c-49f4-aa40-4fc1196bb046",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_run_name = execution_run[\"PipelineExecutionDisplayName\"]\n",
    "print(execution_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3436c82a-9690-4d19-be46-7d1a2ef75a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_arn = execution_run[\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32dff53-28a8-44e0-a821-ba418f620ca3",
   "metadata": {},
   "source": [
    "# List Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b079d73e-979f-4396-a336-338a69376eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Giving the first step time to start up\n",
    "time.sleep(30)\n",
    "\n",
    "execution.list_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a94c96-502d-4077-8229-b5443db7c68a",
   "metadata": {},
   "source": [
    "# Wait for the Pipeline to Complete\n",
    "\n",
    "# _Note: If this cell errors out with `WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded`, just re-run it and keep waiting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d7c30d-5cc0-48ce-9a00-82218d535f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891d8b19-2afa-4ffa-80da-61fbae9237a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)\n",
    "\n",
    "while pipeline_execution_status == \"Executing\":\n",
    "    try:\n",
    "        executions_response = sm.list_pipeline_executions(PipelineName=pipeline_name)[\"PipelineExecutionSummaries\"]\n",
    "        pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "    #        print('Executions for our pipeline...')\n",
    "    #        print(pipeline_execution_status)\n",
    "    except Exception as e:\n",
    "        print(\"Please wait...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "pprint(executions_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e7955-92ef-4094-a37b-01877dab50ca",
   "metadata": {},
   "source": [
    "# Wait for the Pipeline ^^ Above ^^ to Complete\n",
    "\n",
    "# _Note: If this cell errors out with `WaiterError: Waiter PipelineExecutionComplete failed: Max attempts exceeded`, just re-run it and keep waiting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf06c6-772d-45f5-aff6-bd810790575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e775554-5492-49bb-ae63-96fba5fae523",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_arn = executions_response[0][\"PipelineExecutionArn\"]\n",
    "print(pipeline_execution_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bd567-b66d-4983-8233-3cbf84be9409",
   "metadata": {},
   "source": [
    "We can list the execution steps to check out the status and artifacts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c6153-a3c8-46ba-87d2-b489e8cfc627",
   "metadata": {},
   "source": [
    "# List Pipeline Execution Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef486909-4979-4733-a28a-6acbe2c82041",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_execution_status = executions_response[0][\"PipelineExecutionStatus\"]\n",
    "print(pipeline_execution_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab335f98-2707-497b-a10b-37ee6c2a8765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "steps = sm.list_pipeline_execution_steps(PipelineExecutionArn=pipeline_execution_arn)\n",
    "\n",
    "pprint(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49915b8c-aa85-43b3-9418-0b31ca702c18",
   "metadata": {},
   "source": [
    "# List All Artifacts Generated By The Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a609b-af6e-4617-ab4a-5e8e7cb83196",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_name = None\n",
    "training_job_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d1874-50f8-4e7a-ba27-51ee5a96f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.lineage.visualizer import LineageTableVisualizer\n",
    "\n",
    "viz = LineageTableVisualizer(sagemaker.session.Session())\n",
    "\n",
    "for execution_step in reversed(steps[\"PipelineExecutionSteps\"]):\n",
    "    print(execution_step)\n",
    "    # We are doing this because there appears to be a bug of this LineageTableVisualizer handling the Processing Step\n",
    "    if execution_step[\"StepName\"] == \"Processing\":\n",
    "        processing_job_name = execution_step[\"Metadata\"][\"ProcessingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(processing_job_name)\n",
    "        display(viz.show(processing_job_name=processing_job_name))\n",
    "    elif execution_step[\"StepName\"] == \"Train\":\n",
    "        training_job_name = execution_step[\"Metadata\"][\"TrainingJob\"][\"Arn\"].split(\"/\")[-1]\n",
    "        print(training_job_name)\n",
    "        display(viz.show(training_job_name=training_job_name))\n",
    "    else:\n",
    "        display(viz.show(pipeline_execution_step=execution_step))\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987052d-6819-4e84-8710-eca948b54971",
   "metadata": {},
   "source": [
    "# Track Additional Parameters in our Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dba5662-3f87-4fb8-a078-9d290a815e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -aws-processing-job is the default name assigned by ProcessingJob\n",
    "processing_job_tc = \"{}-aws-processing-job\".format(processing_job_name)\n",
    "print(processing_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a174e-d731-4794-86ac-3f59a8c25b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r pipeline_trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef22cd-4b30-4d89-b7ae-7117672ab067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipeline_trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dc7d4a-cd39-44e8-a0cb-7d8c0132acce",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.associate_trial_component(TrialComponentName=processing_job_tc, TrialName=pipeline_trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c4475b-61e8-43cd-ac07-d1d9a06af9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -aws-training-job is the default name assigned by TrainingJob\n",
    "training_job_tc = \"{}-aws-training-job\".format(training_job_name)\n",
    "print(training_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2d4841-7d16-4d1a-9174-1ffa022d2fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = sm.associate_trial_component(TrialComponentName=training_job_tc, TrialName=pipeline_trial_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2f0a07-2a7d-4ed0-b323-6ca89c46b42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments import tracker\n",
    "\n",
    "processing_job_tracker = tracker.Tracker.load(trial_component_name=processing_job_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454cb188-0f47-4181-b400-0afa33c3ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"balance_dataset\": str(balance_dataset),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9013cef-c0db-4bc1-a8ca-2a37cec5a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"train_split_percentage\": str(train_split_percentage),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f3fed-0080-414b-8942-305e67b0d8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"validation_split_percentage\": str(validation_split_percentage),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb445e0d-075d-4ad5-94db-21f33b259b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"test_split_percentage\": str(test_split_percentage),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235912f-b19b-4a8e-bf99-59e130956705",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"max_seq_length\": str(max_seq_length),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60730f-f4a9-42fd-9239-56a0cddad689",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)  # avoid throttling exception\n",
    "\n",
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"feature_store_offline_prefix\": str(feature_store_offline_prefix),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eef919-d5a3-453e-8df8-a898824220e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)  # avoid throttling exception\n",
    "\n",
    "processing_job_tracker.log_parameters(\n",
    "    {\n",
    "        \"feature_group_name\": str(feature_group_name),\n",
    "    }\n",
    ")\n",
    "\n",
    "# must save after logging\n",
    "processing_job_tracker.trial_component.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307fca1-5854-426b-9a2b-b8eb9d0cd7fa",
   "metadata": {},
   "source": [
    "# Analyze Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b73ec3-2405-45a3-a3e5-1272e19414eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "time.sleep(30)  # avoid throttling exception\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"max_colwidth\", 500)\n",
    "\n",
    "experiment_analytics = ExperimentAnalytics(\n",
    "    experiment_name=pipeline_experiment_name,\n",
    ")\n",
    "\n",
    "experiment_analytics_df = experiment_analytics.dataframe()\n",
    "experiment_analytics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
