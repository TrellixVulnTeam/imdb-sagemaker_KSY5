{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.huggingface import HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "bucket = session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = session.boto_region_name()\n",
    "raw_input_path = f\"s3://{bucket}/imdb/processing/raw/small/raw.csv\"\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {bucket}\")\n",
    "print(f\"sagemaker session region: {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce638290-c112-40b9-87a0-3468546274af",
   "metadata": {},
   "source": [
    "## BYO Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458b0ac-71de-47d3-9a46-1697eb8ba674",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc446bfa-db1f-40f6-91c5-8152031b9382",
   "metadata": {},
   "source": [
    "This is the Dockerfile to create the processing container. Install `pandas` and `scikit-learn` into it. You can install your own dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d4e91-914d-4a58-b99d-e5dbb2785c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile docker/Dockerfile\n",
    "\n",
    "FROM python:3.7-slim-buster\n",
    "\n",
    "RUN pip3 install pandas scikit-learn transformers==4.4.2 torch\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "\n",
    "ENTRYPOINT [\"python3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596434e9-6e2d-4079-bdf8-9f6de5e56335",
   "metadata": {},
   "source": [
    "This block of code builds the container using the `docker` command, creates an Amazon Elastic Container Registry (Amazon ECR) repository, and pushes the image to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92892285-b6b1-4f5a-9dc6-2ad0598220b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "ecr_repository = \"sagemaker-pytorch-processing-container\"\n",
    "tag = \":latest\"\n",
    "\n",
    "uri_suffix = \"amazonaws.com\"\n",
    "processing_repository_uri = \"{}.dkr.ecr.{}.{}/{}\".format(\n",
    "    account_id, region, uri_suffix, ecr_repository + tag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bed8c5-a69f-4b05-9002-1aaf2d371f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!docker build -t $ecr_repository docker\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $processing_repository_uri\n",
    "!docker push $processing_repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ae734c-ab17-490a-9c0b-bed59bd151f7",
   "metadata": {},
   "source": [
    "The `ScriptProcessor` class lets you run a command inside this container, which you can use to run your own script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894aa1e1-bdd5-44f0-88b2-a6400fa43d56",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a397e7c-5825-4e3c-bc0f-6f9262806781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "script_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=processing_repository_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba61a6d3-3ad6-4721-9c20-e9d64e7bba60",
   "metadata": {},
   "source": [
    "Run the same `preprocessing.py` script you ran above, but now, this code is running inside of the Docker container you built in this notebook, not the scikit-learn image maintained by Amazon SageMaker. You can add the dependencies to the Docker image, and run your own pre-processing, feature-engineering, and model evaluation scripts inside of this container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08d81f1-238a-4394-a425-e1a339da8d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor.run(\n",
    "    code=\"source/preprocessing.py\",\n",
    "    inputs=[ProcessingInput(source=raw_input_path, destination=\"/opt/ml/processing/input\")],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    arguments=[\"--train-test-split-ratio\", \"0.2\", \"--model_name\", \"distilbert-base-uncased\"],\n",
    ")\n",
    "script_processor_job_description = script_processor.jobs[-1].describe()\n",
    "output_config = script_processor_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"train\":\n",
    "        preprocessed_training_data = output[\"S3Output\"][\"S3Uri\"]\n",
    "    if output[\"OutputName\"] == \"test\":\n",
    "        preprocessed_test_data = output[\"S3Output\"][\"S3Uri\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfb00c-43f3-4810-86bc-db4f488b7baa",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shared-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {'epochs': 1,\n",
    "                   'per_device_train_batch_size': 32,\n",
    "                   'model_name': 'distilbert-base-uncased'\n",
    "                   }\n",
    "\n",
    "# create the Estimator\n",
    "estimator = HuggingFace(\n",
    "    entry_point='train.py',\n",
    "    source_dir='./source',\n",
    "    instance_type='ml.g4dn.xlarge',  # Note: needs to be an instance with gpu\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    transformers_version='4.4',\n",
    "    pytorch_version='1.6',\n",
    "    py_version='py36',\n",
    "    hyperparameters=hyperparameters\n",
    ")\n",
    "estimator.fit({'train': preprocessed_training_data, 'test': preprocessed_test_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678dc09-138c-4187-8c1b-1b677e1cf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_data = estimator.model_data\n",
    "model_data = 's3://sagemaker-eu-central-1-611215368770/pytorch-training-2021-06-03-16-49-22-917/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f31684-e8aa-4de9-ab2d-b635760fd4d8",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "`evaluation.py` is the model evaluation script. Since the script also runs using scikit-learn as a dependency,  run this using the `SKLearnProcessor` you created previously. This script takes the trained model and the test dataset as input, and produces a JSON file containing classification evaluation metrics, including precision, recall, and F1 score for each label, and accuracy and ROC AUC for the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d45f98-72b6-4df5-a822-033f35605093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "script_processor.run(\n",
    "    code=\"source/evaluation.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(source=model_data, destination=\"/opt/ml/processing/model\"),\n",
    "        ProcessingInput(source=preprocessed_test_data, destination=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\")],\n",
    ")\n",
    "evaluation_job_description = script_processor.jobs[-1].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ef0289-1817-4a15-a73b-699cfb0a872b",
   "metadata": {},
   "source": [
    "Now retrieve the file `evaluation.json` from Amazon S3, which contains the evaluation report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52687677-390f-471c-84fb-0fa9e828cb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_output_config = evaluation_job_description[\"ProcessingOutputConfig\"]\n",
    "for output in evaluation_output_config[\"Outputs\"]:\n",
    "    if output[\"OutputName\"] == \"evaluation\":\n",
    "        evaluation_s3_uri = output[\"S3Output\"][\"S3Uri\"] + \"/evaluation.json\"\n",
    "        break\n",
    "\n",
    "evaluation_output = S3Downloader.read_file(evaluation_s3_uri)\n",
    "evaluation_output_dict = json.loads(evaluation_output)\n",
    "print(json.dumps(evaluation_output_dict, sort_keys=True, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}